---
title: "Project_2_Group_3"
author: "Logan Thomson"
date: "11/26/2017"
output: html_document
---

&^& Add transformations
%$% Run models

##LOAD PACKAGES  

```{r load_pkgs, message=FALSE}
library(e1071)
library(dplyr)
library(tidyr)
library(ggplot2)
library(VIM)
library(corrplot)
```  

###LOAD DATA  

Loading data from GitHub:  

```{r}
# open file
path <- ("https://raw.githubusercontent.com/kennygfm/fall624-group3/master/StudentData.csv")
con <- file(path, open="r")

# "Student" soft drink data
soda <- read.csv(con, header=T, sep=",", stringsAsFactors = F)

# close file
close(con)

soda[ , c(16,18,21,28)] <- sapply(soda[, c(16,18,21,28)], as.numeric)  # get rid of pesky integer values
```  

Dataset is 32 predictors + 1 target variable (`Brand.Code`), with 2571 observations:  

```{r dim}
dim(soda) 
```  

With the exception of the `Brand.Code`, all variables are either numeric or integers:  

```{r data_structure}
str(soda)
```  

##EXPLORE DATA  

####Summary Table:  

There are too many predictors for the standard `summary` function to produce an easy-to-read output. A more useful summary with skewness, correlations to `PH`, and `NA` counts is below. The variable `Brand.Code`, because it is a character vector, is left out:  

```{r summary_table}
means <- sapply(soda[-1], function(y) mean(y, na.rm = TRUE))
medians <- sapply(soda[-1], function(y) median(y, na.rm = TRUE))
IQRs <- sapply(soda[-1], function(y) IQR(y, na.rm = TRUE))
vars <- sapply(soda[-1], function(y) var(y, na.rm = TRUE))
skews <- sapply(soda[-1], function(y) skewness(as.numeric(y), na.rm = TRUE))
cors <- as.vector(cor(soda$PH, soda[,2:ncol(soda)], use = "complete.obs"))
NAs <- sapply(soda[-1], function(y) sum(length(which(is.na(y)))))

soda_summary <- data.frame(means, medians, IQRs, vars, skews, cors, NAs)
colnames(soda_summary) <- c("MEAN", "MEDIAN", "IQR", "Var", "SKEW", "$r_{PH}$", "NAs")
soda_summary <- round(soda_summary, 2)

soda_summary
```  

The table above shows some useful information regarding the mean and median for each predictor, and comparing the differences between the two, skewness of some variables is already apparent. Using the `skewness` function from `e1071`, the predictors `MFR`, `Filler.Speed`, `Carb.Flow`, and `Bowl.Setpoint` are shown to be the most negatively skewed. On the other end of the spectrum, `Oxygen.Filler`, `Temperature`, `Air.Pressurer`, `PSC.Fill`, and `PSC.CO2` are all positively skewed. Many of these predictors have to do with gasses and air pressure, and we may want to consider scaling and centering these and other predictors before fitting any models.  

Boxplots of these predictors are provided below:  

####Boxplots:  

```{r skewed_preds}
skewed_preds <- soda[, c(8,9,18,19,21:23,27,28,30)]

s <- gather(skewed_preds, "predictor", "value")

ggplot(s, aes(predictor, value)) + geom_boxplot(aes(fill = predictor)) + facet_wrap(~predictor, scale="free") + scale_fill_discrete(guide=FALSE) + scale_y_continuous('', labels = NULL, breaks = NULL) +
  scale_x_discrete('') + ggtitle("Most Skewed Predictors")
```  

####Histograms  

Aside from the skewed predictors, histograms of all 32 numeric predictors are created in order to get a quick overview of the distributions, as well as to spot any odd patterns:  

```{r hist_set1}
par(mfrow=c(4,4))

for (i in c(2:17)){
  hist(soda[ ,i], main = paste(names(soda[i]), "Distribution"), 
       xlab = names(soda)[i], cex.main = 0.9, cex.axis = 0.7,
       col=rgb(64, 224, 208, max = 255, alpha = 70, names = "grays"), pch=19)
}
par(mfrow=c(1,1))
```  

Many of the first 16 predictors (as ordered in the dataset) have normal or somewhat-normal distributions, and appear to be continuous variables. A few of the skewed distributions follow an almost chi-squared or log-normal distribution, so these are predictors where transformations should be considered (`PSC.Distribution` and `PSC.C02.Distribution`). 

Another insteresting pattern are the number of 0 values in the `Hyd.Pressure`(1-3) variables.  The $4^{th}$ `Hyd.Pressure` does not follow this same pattern, so depending on the relationship between these variables and the target and/or other variables, they may be removed outright.  

The next set of histograms for variables 17-32 are below:  

```{r hist_set2}
par(mfrow=c(4,4))

for (i in c(18:ncol(soda))){
  hist(soda[ ,i], main = paste(names(soda[i]), "Distribution"), 
       xlab = names(soda)[i], cex.main = 0.9, cex.axis = 0.7,
       col=rgb(64, 224, 208, max = 255, alpha = 70, names = "grays"), pch=19)
}
par(mfrow=c(1,1))
```  

These next predictors follow some different patterns.  A few are normal or near-normal distributions (`Pressure.Vacuum` and `PH`, the target), and some are highly skewed (`Filler.Speed`, `Oxygen.Filler`). Another set of predictors appear to not be continuous, but discrete distributions (`Pressure.Setpoint`, `Alch.Rel`), however, as we'll see in later plots, many of these variables are just constrained to a few values, but are still continuous.  

Out of all the variables, `Bowl.Setpoint` does seem to be a discrete distribution:  

```{r}
table(soda$Bowl.Setpoint)
```  

```{r bowl_setpt_dist}
hist(soda$Bowl.Setpoint, main="Bowl.Setpoint", col='grey', xlab='Value')
```  

####Correlated Predictors:  

Aside from skewed variables, several variables have high correlations with each other:  

```{r cor_plt}
cors_all_preds <- round(cor(soda[-1], use="complete.obs"), 2)

cors_all_preds_df <- as.data.frame(cors_all_preds)

corrplot(as.matrix(cors_all_preds_df), method="color", tl.cex=.5, tl.col=colors()[598])
```  

As the columns are organized in the data, some interesting patterns are present in the correlogram.  Two areas show distinct positive correlations - these are the predictors that have something to do with carbonation, and another area where different pressure levels correlate with each other. Another set of variables are negatively correlated with these pressure predictors, these have to do with the filling of the bottles, so this makes sense (`Oxygen.Filler`, `Bowl.Setpoint`, `Pressure.Setpoint`).  

Some of these same precictors are also correlated well with the target `PH` variable:  

```{r tgt_corr_var}
vars <- rownames(cors_all_preds)

top_ph_cors <- cors_all_preds_df$PH

top_ph_cors <- as.data.frame(cbind(vars, as.numeric(as.character(top_ph_cors))))

top_ph_cors$V2 <- as.numeric(as.character(top_ph_cors$V2))

top_ph_cors_neg <- top_ph_cors %>%
  arrange(V2)

top_ph_cors_pos <- top_ph_cors %>%
  arrange(desc(V2))
```  

Below are the top variables that are positively correlated with the target `PH` variable:  

```{r}
top_ph_cors_pos[2:11, ]
``` 

Without transforming predictors, many of the highly correlated predictors to `PH` have to do with the bottle filling process (Fillers, Flow, Vacuums, etc.).  In addition, the predictors that have the highest negative correlation to `PH` are below:  

```{r}
top_ph_cors_neg[1:11, ]
```  

Again, the same pattern as the positively correlated predictors is apparent. It may be worth constructing some models only using these predictors, as using all 32 variables may prove to introduce a lot of noise into the model.  

#####Intercorrelated Predictors

In addition, some of the variables are highly correlated with each other. The following pairings seem to have something to do with each other in the bottle filling process:  

```{r inter_corr}
cors_0_diag <- cors_all_preds  # create duplicate matrix

cors_0_diag[lower.tri(cors_0_diag, diag = TRUE)] <- NA  # prevent duplicates by taking upper triangle of matrix

cors_0_diag_df <- as.data.frame(cors_0_diag)


inter_cor <- reshape2::melt(as.matrix(cors_0_diag_df))  # melt into single col of correlations

colnames(inter_cor)[3] <- "Correlation" 

neg_cors <- inter_cor %>%
  arrange(Correlation)

pos_cors <- inter_cor %>%
  arrange(desc(Correlation))
```  

The following predictors have high negative correlations with each other

```{r neg_intercor}
neg_cors[1:15, ]
```  

Conversely, the following predictors are postively correlated with each other. On this end of the spectrum, many of the predictors are almost perfectly correlated with one another. We may want to consider removing some of these redundant variables, perhaps the ones that are less correlated to the target variable:  

```{r pos_intercor}
pos_cors[1:15, ]
```  

####Bi-Variate Exploration  

**Scatterplots Between PH and Other Predictors**  

Before moving on to modeling, we'll look further into the relationship between the `PH` target variable and the other predictors. The first predictors we'll examine against `PH` are the most highly correlated ($+$ or $-$):  

```{r hi_cor_scatplt}
par(mfrow=c(2,3))
for (i in c(10,12,17,20,28,29)){
  plot(soda[ ,i], soda$PH, main = paste(names(soda[i]), "vs. PH"), 
       xlab = names(soda)[i], ylab = "Ph", cex.main = 0.9, cex.lab = 0.7,
       col=rgb(112, 128, 144, max = 255, alpha = 70, names = "grays"), pch=19)
}
par(mfrow=c(1,1))
```  

```{r cor_pairs}
pairs(soda[ ,c(10,12,17,20,28,29)], main='Variables Highly Correlated with Target', 
      col=rgb(0, 0, 255, max = 255, alpha = 95, names = "blue"), pch=16)
```  

The remaining predictors are plotted against the `PH` variable to see if any relevant patterns emerge:  

```{r vars_2thru9}
par(mfrow=c(2,4))

for (i in c(2:9)){
  plot(soda[ ,i], soda$PH, main = paste(names(soda[i]), "vs. PH"), 
       xlab = names(soda)[i], ylab = "Ph",  cex.main = 0.9, cex.lab = 0.7,
       col=rgb(255, 99, 71, max = 255, alpha = 70, names = "reds"), pch=19)
}
par(mfrow=c(1,1))
```  

```{r vars_11thru21}
par(mfrow=c(2,4))

# c(10,12,17,20,28,29) already used

for (i in c(11,13:16,18,19,21)){
  plot(soda[ ,i], soda$PH, main = paste(names(soda[i]), "vs. PH"), 
       xlab = names(soda)[i], ylab = "Ph",  cex.main = 0.9, cex.lab = 0.7,
       col=rgb(64, 224, 208, max = 255, alpha = 70, names = "grays"), pch=19)
}
par(mfrow=c(1,1))
```  

```{r vars_22thru32}
par(mfrow=c(2,4))

for (i in c(22:25, 27, 30, 31, 32)){
  plot(soda[ ,i], soda$PH, main = paste(names(soda[i]), "vs. PH"), 
       xlab = names(soda)[i], ylab = "Ph", cex.main = 0.9, cex.lab = 0.7,
       col=rgb(221, 160, 221, max = 255, alpha = 70, names = "reds"), pch=19)
}
par(mfrow=c(1,1))
```  

While no strong patters emerge, there is some directionality in the scatterplots for a few predictors, such as `Pressure.Vacuum`, `Bowl.Setpoint`, `Air.Pressurer` and `Alch.Rel`.  While only one of these variables is from the set highly correlated with the predictor, all of these should probably be considered for any model.  

####Missing Values:   

One of the predictors (`MFR`) contains a considerable amount of missing values (~$8 \%$ of the cases). This variable is also highly skewed, so imputing using only the median/mean should be done with care, or other methods investigated:  

```{r missing_vals}
sort(NAs, decreasing = TRUE)
``` 

```{r mfr_hist}
hist(soda$MFR, col='Blue', main='Distribution of MFR Predictor')
```

Most of the other predictors are missing values for a marginal number of cases ($< 3\%$), but a few cases are missing data across a number of the predictors.  Instead of removing entire variables or imputing them all, we may want to remove some of the cases from the data instead. The plot below shows the ratio and location of the missing values within the dataset:  

```{r missing_values, message=FALSE, warning=FALSE}
aggr(soda, col=c('navyblue', 'red'), numbers=TRUE, cex.numbers=.75, cex.axis=0.5)
```  

Counts of the different brand codes are below:  

```{r brand_counts}
table(soda$Brand.Code)
```  

Most of the cases (nearly half) are brand "B"; the next most common is brand "D", followed by "C", and "A" closely behind that.  120 ($> 5 \%$) of the cases in the soda dataset contain no brand name. Whether these are all the same brand will require some exploration.  


```{r blank_brand}
brand_unknown <- soda[soda$Brand.Code == '', ]
```  

The same method used to create the overall summary for all brands is used for the separated "unknown" or blank brand name. The minimum and maximum have been added to show the range in addition to the IQR for each predictor within this subset:    

```{r unkwn_brand_summ}
means <- sapply(brand_unknown[-1], function(y) mean(y, na.rm = TRUE))
medians <- sapply(brand_unknown[-1], function(y) median(y, na.rm = TRUE))
IQRs <- sapply(brand_unknown[-1], function(y) IQR(y, na.rm = TRUE))
skews <- sapply(brand_unknown[-1], function(y) skewness(as.numeric(y), na.rm = TRUE))
NAs <- sapply(brand_unknown[-1], function(y) sum(length(which(is.na(y)))))

brand_unknown_summary <- data.frame(means, medians, IQRs, skews, NAs)
colnames(brand_unknown_summary) <- c("MEAN", "MEDIAN", "IQR", "SKEW", "NAs")
brand_unknown_summary <- round(brand_unknown_summary, 2)

brand_unknown_summary
```  

Examining the results, there is not too much variance in the predictors that actually rely on the makeup (chemical or otherwise) of the soda itself. These would be predictors like `Fill.Ounces`, `PH`, and `Carb.Volume`.  The predictors that have more variance have to do with temperature and pressure, and given the changes in these across all predictors, it is probably safe to assume that the unknown brand is just un-named.  

Just to make the case, we'll look at the same stats for another brand ("D"), just to see if the same predictors stay consistent, or if we should expect more variance:  

```{r brand_d}
brand_d <- soda[soda$Brand.Code == 'D', ]
```  

```{r brand_d_summ}
mins <- sapply(brand_d[-1], function(y) min(y, na.rm = TRUE))
means <- sapply(brand_d[-1], function(y) mean(y, na.rm = TRUE))
medians <- sapply(brand_d[-1], function(y) median(y, na.rm = TRUE))
IQRs <- sapply(brand_d[-1], function(y) IQR(y, na.rm = TRUE))
maxs <- sapply(brand_d[-1], function(y) max(y, na.rm = TRUE))
skews <- sapply(brand_d[-1], function(y) skewness(as.numeric(y), na.rm = TRUE))
cors <- as.vector(cor(brand_d$PH, brand_d[,2:ncol(brand_d)], use = "complete.obs"))
NAs <- sapply(brand_d[-1], function(y) sum(length(which(is.na(y)))))

brand_d_summary <- data.frame(mins, means, medians, IQRs, maxs, skews, cors, NAs)
colnames(brand_d_summary) <- c("MIN", "MEAN", "MEDIAN", "IQR", "MAX", "SKEW", "$r_{PH}$", "NAs")
brand_d_summary <- round(brand_d_summary, 2)

brand_d_summary
```  

####Brand and PH Correlation  

Before removing the `Brand.Code` predictor, we'll check for any correlation between the brand name/type and the target variable (`PH`). The character data is replaced with arbitrary numerical values:  

```{r brand_replace}
soda$Brand.Code[which(soda$Brand.Code == "")] <- 1
soda$Brand.Code[which(soda$Brand.Code == "A")] <- 2
soda$Brand.Code[which(soda$Brand.Code == "B")] <- 3
soda$Brand.Code[which(soda$Brand.Code == "C")] <- 4
soda$Brand.Code[which(soda$Brand.Code == "D")] <- 5

soda$Brand.Code <- as.numeric(soda$Brand.Code)
```  

After replacing the characters, the correlation of the brands to `PH` level is calculated.  There is very little correlation, either positive or negative, between the brand of soda and the Ph level.  

```{r}
brand_ph <- as.data.frame(cbind(soda$Brand.Code, soda$PH))

colnames(brand_ph) <- c("BrandCode", "PH")

cors_brand_ph <- round(cor(brand_ph, use="complete.obs"), 2)

cors_brand_ph <- as.data.frame(cors_brand_ph)

corrplot(as.matrix(cors_brand_ph), method="color", tl.cex=.75, tl.col=colors()[598])
```  


